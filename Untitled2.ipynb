{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab17625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014933d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3795, 2048, 3)\n",
      "Dataset contents:\n",
      " [[[ 0.05386386 -0.05882462 -0.13248975]\n",
      "  [ 0.15363191 -0.02008284 -0.03307019]\n",
      "  [-0.06529021 -0.04172364 -0.37278084]\n",
      "  ...\n",
      "  [ 0.3578785  -0.00841344  0.00908047]\n",
      "  [-0.40297264 -0.0177596  -0.03205029]\n",
      "  [-0.02186051 -0.04808117  0.27173295]]\n",
      "\n",
      " [[-0.0814341   0.07964655  0.0466872 ]\n",
      "  [-0.34620114  0.07316021 -0.07315259]\n",
      "  [-0.13113066  0.00275466 -0.32854486]\n",
      "  ...\n",
      "  [-0.24122708  0.07472612  0.05962841]\n",
      "  [ 0.35068104 -0.02784328  0.00312881]\n",
      "  [-0.04476729  0.00077407 -0.12927641]]\n",
      "\n",
      " [[-0.16958506 -0.15945585  0.22989281]\n",
      "  [-0.35869471 -0.07443494  0.06870167]\n",
      "  [-0.4548238  -0.08651434 -0.02378498]\n",
      "  ...\n",
      "  [-0.3123181  -0.08896376 -0.08889477]\n",
      "  [ 0.02879597 -0.15949575 -0.26985056]\n",
      "  [-0.18440187 -0.15954732 -0.41293134]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.3853267  -0.04612798 -0.01913776]\n",
      "  [-0.34320257 -0.0189235  -0.30001923]\n",
      "  [-0.24802673 -0.05575589 -0.09189186]\n",
      "  ...\n",
      "  [-0.33997184 -0.04061643 -0.26165857]\n",
      "  [-0.22029353 -0.04133505 -0.21767636]\n",
      "  [-0.40146003 -0.05834585 -0.11454919]]\n",
      "\n",
      " [[-0.14425427 -0.05021042  0.43311134]\n",
      "  [ 0.44379003 -0.08517654 -0.01714428]\n",
      "  [-0.01843848 -0.07454007  0.16739738]\n",
      "  ...\n",
      "  [-0.39135946  0.08590038 -0.00245075]\n",
      "  [-0.37240332 -0.02203425 -0.04728749]\n",
      "  [-0.47487602  0.10574785 -0.00190187]]\n",
      "\n",
      " [[-0.41803694 -0.01862667 -0.26898962]\n",
      "  [-0.42007497  0.01598666 -0.2689876 ]\n",
      "  [-0.41792781 -0.04055138 -0.28352943]\n",
      "  ...\n",
      "  [-0.33605286 -0.01728526 -0.26911955]\n",
      "  [-0.29165755  0.00412673  0.02095622]\n",
      "  [ 0.42823187 -0.03880165 -0.01494907]]]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'processed_datasets/plane.npy'  \n",
    "dataset = np.load(file_path)\n",
    "\n",
    "print(\"Dataset shape:\", dataset.shape)\n",
    "print(\"Dataset contents:\\n\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20f895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3795, 2048, 3)\n",
      "Dataset contents:\n",
      " [[[ 0.10056132 -0.06630287  0.04257465]\n",
      "  [ 0.24284947 -0.09077123 -0.02270481]\n",
      "  [-0.08373262 -0.03815453 -0.45995328]\n",
      "  ...\n",
      "  [ 0.06266463 -0.08682963  0.1585905 ]\n",
      "  [ 0.14066664 -0.02805065  0.03834667]\n",
      "  [-0.01278953 -0.07149194 -0.09816316]]\n",
      "\n",
      " [[ 0.27923814 -0.03706354 -0.02390867]\n",
      "  [-0.08380569 -0.03825525  0.04495752]\n",
      "  [-0.26596675  0.0737177   0.03777526]\n",
      "  ...\n",
      "  [-0.23614092 -0.01241465 -0.45582994]\n",
      "  [-0.30420616 -0.01241465 -0.00421277]\n",
      "  [-0.00509406  0.0792618  -0.06161284]]\n",
      "\n",
      " [[-0.20109513 -0.14130771  0.04661819]\n",
      "  [-0.18101659 -0.15946116 -0.32157826]\n",
      "  [-0.10066256 -0.17408037 -0.25965737]\n",
      "  ...\n",
      "  [ 0.10516994 -0.15946116  0.11968247]\n",
      "  [ 0.32295781 -0.11082889  0.05545796]\n",
      "  [ 0.06287071 -0.11631404 -0.0643444 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.41866927 -0.03282138 -0.02263017]\n",
      "  [ 0.03639762  0.00048039 -0.01986511]\n",
      "  [ 0.38246144 -0.05347661  0.00240172]\n",
      "  ...\n",
      "  [-0.38392986 -0.04113708 -0.09582743]\n",
      "  [-0.28152544 -0.03051352  0.160055  ]\n",
      "  [-0.44397923 -0.02971253  0.06058835]]\n",
      "\n",
      " [[ 0.04047504 -0.10025514  0.28977771]\n",
      "  [-0.39507143  0.02270253 -0.00626142]\n",
      "  [-0.19429969 -0.00620815 -0.01176059]\n",
      "  ...\n",
      "  [-0.03321522 -0.07428176 -0.20875098]\n",
      "  [-0.45360605 -0.01904149 -0.08833161]\n",
      "  [-0.00698561 -0.08955632 -0.07003409]]\n",
      "\n",
      " [[-0.01969169 -0.03901278  0.02752452]\n",
      "  [ 0.29883903 -0.03912562 -0.01114929]\n",
      "  [ 0.30015962 -0.02730166  0.02443921]\n",
      "  ...\n",
      "  [-0.43479664 -0.04040916 -0.45305481]\n",
      "  [-0.41998399 -0.04040916 -0.46979403]\n",
      "  [-0.35899457 -0.02034246  0.04669555]]]\n"
     ]
    }
   ],
   "source": [
    "file_path_gt = 'processed_datasets_gt/plane.npy' \n",
    "dataset_gt = np.load(file_path_gt)\n",
    "\n",
    "print(\"Dataset shape:\", dataset_gt.shape)\n",
    "print(\"Dataset contents:\\n\", dataset_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575a91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnhancedPointCloudEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedPointCloudEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 1)\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.max(x, 2)[0]  # Global max pooling\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class EnhancedPointCloudDecoder(nn.Module):\n",
    "    def __init__(self, num_points=2048):\n",
    "        super(EnhancedPointCloudDecoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_points * 3)  # Output layer for generating points\n",
    "        self.num_points = num_points\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, 3, self.num_points)  # Reshape to (batch_size, 3, num_points)\n",
    "        return x\n",
    "\n",
    "class EnhancedPointCloudCompletionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedPointCloudCompletionModel, self).__init__()\n",
    "        self.encoder = EnhancedPointCloudEncoder()\n",
    "        self.decoder = EnhancedPointCloudDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e9e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectedPointCloudCompletionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipConnectedPointCloudCompletionModel, self).__init__()\n",
    "        self.encoder = EnhancedPointCloudEncoder()\n",
    "        self.decoder = EnhancedPointCloudDecoder()\n",
    "\n",
    "        # Skip connections\n",
    "        self.skip_conv1 = nn.Conv1d(64, 64, 1)\n",
    "        self.skip_conv2 = nn.Conv1d(128, 128, 1)\n",
    "        self.skip_conv3 = nn.Conv1d(256, 256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.encoder.conv1(x))\n",
    "        x2 = F.relu(self.encoder.conv2(x1))\n",
    "        x3 = F.relu(self.encoder.conv3(x2))\n",
    "        \n",
    "        x_encoded = torch.max(x3, 2)[0]  # Global max pooling\n",
    "        x_encoded = F.relu(self.encoder.fc1(x_encoded))\n",
    "        x_encoded = F.relu(self.encoder.fc2(x_encoded))\n",
    "\n",
    "        # Use skip connections for decoder\n",
    "        x = self.decoder.fc1(x_encoded)\n",
    "        x = self.skip_conv3(x) + x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.decoder.fc2(x)\n",
    "        x = self.skip_conv2(x) + x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.decoder.fc3(x)\n",
    "        x = self.skip_conv1(x) + x\n",
    "        x = x.view(-1, 3, self.decoder.num_points)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming train_data and gt_data are loaded and permuted properly\n",
    "\n",
    "train_data = torch.tensor(dataset, dtype=torch.float32).permute(0, 2, 1)  # (batch_size, 2048, 3) -> (batch_size, 3, 2048)\n",
    "gt_data = torch.tensor(dataset_gt, dtype=torch.float32).permute(0, 2, 1)        # (batch_size, 2048, 3) -> (batch_size, 3, 2048)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset1 = TensorDataset(train_data, gt_data)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_loader = DataLoader(dataset1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = SkipConnectedPointCloudCompletionModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "betas = torch.linspace(0.1, 0.5, 500).to(device)\n",
    "\n",
    "# Training and evaluation loop\n",
    "def train(model, train_loader, optimizer, epochs=100):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "                \n",
    "                batch = batch[0].to(next(model.parameters()).device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch)\n",
    "                loss = chamfer_distance(batch, output)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Avg Loss: {epoch_loss:.6f}')\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 5:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    total_chamfer_distance = 0.0\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                batch = batch[0].to(next(model.parameters()).device)\n",
    "                reconstructed_output = model(batch)\n",
    "\n",
    "                cd = chamfer_distance(batch, reconstructed_output)\n",
    "                total_chamfer_distance += cd.item()\n",
    "\n",
    "                # Reshape the output to (batch_size, 2048, 3)\n",
    "                reshaped_output = reconstructed_output.permute(0, 2, 1).cpu()\n",
    "                all_outputs.append(reshaped_output)\n",
    "\n",
    "                tepoch.set_postfix(chamfer_distance=cd.item())\n",
    "\n",
    "    avg_chamfer_distance = total_chamfer_distance / len(test_loader)\n",
    "    print(f'Average Chamfer Distance: {avg_chamfer_distance:.6f}')\n",
    "\n",
    "    # Concatenate all outputs and reshape them to (2048, 3) for each point cloud\n",
    "    all_outputs = torch.cat(all_outputs, dim=0)  # Shape: (total_samples, 2048, 3)\n",
    "\n",
    "    return avg_chamfer_distance, all_outputs\n",
    "\n",
    "# Train and evaluate\n",
    "train(model, train_loader, optimizer, epochs=50)\n",
    "avg_cd, predicted_outputs = evaluate(model, train_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
